version: "3.7"
services:  
    # postgres usado pelo airflow e pela solucao
    postgres:
        image: postgres:9.6
        networks:
            - default_net
        volumes: 
            # cria base de dados test para usarmos na solucao
            - ./docker-airflow/pg-init-scripts:/docker-entrypoint-initdb.d
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        ports:
            - "5432:5432"

    # airflow
    airflow-webserver:
        build:
            context: ./docker-airflow/  #indica o dir da Dockerfile
            dockerfile: Dockerfile
        #image: docker-airflow-spark:1.10.7_3.1.2
        restart: always
        networks:
            - default_net
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
        volumes:
            - ../dags:/usr/local/airflow/dags #Pasta de DAG
            - ../spark/app:/usr/local/spark/app #Scripts python spark (deve ser dentro da pasta do spark app)
            - ../spark/resources:/usr/local/spark/resources #Pasta de resources do spark
            - ../lake:/usr/local/lake #Pasta do datalake
        ports:
            - "8282:8282"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    # Spark com 3 workers
    spark:
        image: bitnami/spark:3.1.2
        user: root # Run container as root container: https://docs.bitnami.com/tutorials/work-with-non-root-containers/
        hostname: spark
        networks:
            - default_net
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ../spark/app:/usr/local/spark/app #Scripts python spark (deve ser dentro da pasta do spark app)
            - ../spark/resources:/usr/local/spark/resources #Pasta de resources do spark
            - ../lake:/usr/local/lake #Pasta do datalake
        ports:
            - "8181:8080"
            - "7077:7077"

    spark-worker-1:
        image: bitnami/spark:3.1.2
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ../spark/app:/usr/local/spark/app #Scripts python spark (deve ser dentro da pasta do spark app)
            - ../spark/resources:/usr/local/spark/resources #Pasta de resources do spark
            - ../lake:/usr/local/lake #Pasta do datalake

    spark-worker-2:
        image: bitnami/spark:3.1.2
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ../spark/app:/usr/local/spark/app #Scripts python spark (deve ser dentro da pasta do spark app)
            - ../spark/resources:/usr/local/spark/resources #Pasta de resources do spark
            - ../lake:/usr/local/lake #Pasta do datalake

    spark-worker-3:
        image: bitnami/spark:3.1.2
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ../spark/app:/usr/local/spark/app #Scripts python spark (deve ser dentro da pasta do spark app)
            - ../spark/resources:/usr/local/spark/resources #Pasta de resources do spark
            - ../lake:/usr/local/lake #Pasta do datalake

    #Jupyter notebook
    jupyter-spark:
        image: jupyter/pyspark-notebook:spark-3.1.2
        networks:
            - default_net
        ports:
            - "8888:8888"
            - "4040-4080:4040-4080"
        volumes:
            - ../notebooks:/home/jovyan/work/notebooks/ #pasta onde ficam os notebooks jupyter
            - ../spark/resources/data:/home/jovyan/work/data/ #Pasta de resources do spark
            - ../spark/resources/jars:/home/jovyan/work/jars/ #Pasta de resources do spark
            - ../lake:/usr/local/lake #Pasta do datalake
    #PG admin
    pgadmin:
        image: dpage/pgadmin4
        environment:
            PGADMIN_DEFAULT_EMAIL: "user@pgadmin.com"
            PGADMIN_DEFAULT_PASSWORD: "PgAdmin@c4680"
        ports:
            - "16543:80"
        depends_on:
            - postgres
        networks:
            - default_net

networks:
    default_net: